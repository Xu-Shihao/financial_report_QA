{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hyde_sys_prompt = \"\"\"\n",
    "# Character\n",
    "You're a financial answer guesser. Even without the latest news or a complete knowledge base, your can produce a simple answer utilizing placeholders such as \"xxx\", \"xxx%\", \"$xxx\", and so on for unknown elements.\n",
    "\n",
    "## Constraints:\n",
    "- Aim to provide the simple reply, even when the latest information is not readily available.\n",
    "- The number of the letter of the answer is limited to 30 words.\n",
    "\n",
    "## Example\n",
    "#Example1\n",
    "--------\n",
    "Question: What will Apple's market value be in 2023?\n",
    "Reply: Apple's market cap in 2023 is $xx billion.\n",
    "\n",
    "#Example2\n",
    "--------\n",
    "Question: How much does Meta's AI investment increase in 2023 compared to 2022?\n",
    "Reply: Meta will invested $xx billion in AI in 2023 and $xx billion in 2022, an increase of xx%.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "QA_sys_prompt = \"\"\"\n",
    "# Character\n",
    "You're a skilled chatbot, capable of extracting relevant information from retrieved documents. When a user poses a question, you answer it as though you have firsthand knowledge rather than referencing a document.\n",
    "\n",
    "## Skills\n",
    "### Skill 1: Answer questions about the retrieved document\n",
    "- Understand the user's question.\n",
    "- Analyze the retrieved document to find relevant information.\n",
    "\n",
    "### Skill 2: Reply when unable to answer\n",
    "- If the question can't be answered based on the document, respond, \"Sorry, I do not have an accurate answer for this.\"\n",
    "\n",
    "## Constraints\n",
    "- Mimic the tone and language used by a chatbot.\n",
    "- Do not reference any document or outside source in your answers.\n",
    "- If no accurate answer can be provided, be honest and inform the user.\n",
    "\"\"\"\n",
    "\n",
    "QA_user_prompt = \"\"\"\n",
    "## Reference document\n",
    "{}\n",
    "\n",
    "Answer this query: {} Make the answer short and clean.\n",
    "\"\"\"\n",
    "\n",
    "verifier_sys_prompt = \"\"\"## Role: Answer verifier\n",
    "\n",
    "## Goal\n",
    "You can judge whether the answer is correct or not. \n",
    "\n",
    "## Rule\n",
    "- If the key information predicted answer is same as the ground truth answer, then the answer is correct.\n",
    "- If the response is \"Sorry, I do not have an accurate answer for this.\", it means the answer can not be found, then the answer can be treated as correct.\n",
    "\n",
    "## Output format\n",
    "{{\n",
    "\"reason\": \"fill the reason why the predicted answer is wrong (False) or correct (True).\", \n",
    "\"answer\": True or False\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "verifier_user_prompt = \"\"\"\n",
    "The question is: {}\n",
    "Ground truth is: {}\n",
    "Predicted answer is: {}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_llm(system_prompt, user_prompt):\n",
    "    from openai import OpenAI\n",
    "\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo-preview\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        temperature=0,\n",
    "        top_p = 0.9\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# gpt_llm(\"You are a helpful assistant.\", \"Who are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    500      1000     1500\n",
      "BM25                                 1.0  0.97619  0.97619\n",
      "mixedbread-ai/mxbai-embed-large-v1   1.0  0.97619  0.97619\n",
      "BAAI/bge-large-en-v1.5               1.0  0.97619  0.97619\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "import pandas as pd\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "import pickle\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())  # read local .env file\n",
    "qa_full_dataset_name = \"QA_dataset_v2\"  # Define dataset name\n",
    "TOP_K = 5\n",
    "\n",
    "def save_pk(chunks, file_path):\n",
    "    with open(file_path, \"wb\") as file:\n",
    "        pickle.dump(chunks, file)\n",
    "\n",
    "def load_pk(file_path):\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "def load_doc_chunks(chunk_size, qa_full_dataset_name):\n",
    "    \n",
    "    chunks = []\n",
    "    file_path = f\"./chunks/chunks_{chunk_size}_{qa_full_dataset_name}.pkl\" \n",
    "    \n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_size//5,\n",
    "        length_function=len\n",
    "    )\n",
    "    try:\n",
    "        # Check if the chunks file already exists\n",
    "        chunks = load_pk(file_path)\n",
    "        print(\"Loaded chunks from existing file. \", file_path)\n",
    "    except FileNotFoundError:\n",
    "        # If the file does not exist, process the PDFs\n",
    "        chunks = []\n",
    "        for pdf in tqdm(glob.glob(\"./docs/*.pdf\")):\n",
    "            pdf_reader = PdfReader(pdf)\n",
    "            text = \"\"\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text() if page.extract_text() else \"\"\n",
    "            chunks += text_splitter.split_text(text)\n",
    "        \n",
    "        # Save the chunks to a file after processing\n",
    "        save_pk(chunks, file_path)\n",
    "        print(\"Saved new chunks to file. \", file_path)\n",
    "    return chunks\n",
    "        \n",
    "def kb_initialization(model_names, chunk_size):\n",
    "    \n",
    "    chunks = []   \n",
    "    retrievers = []\n",
    "    for model_name in model_names:\n",
    "        \n",
    "        # define retriever saving path\n",
    "        index_filename = f\"faiss_index_cs-{chunk_size}_\" + model_name.split(\"/\")[-1]\n",
    "        index_path = \"./faiss/\" + index_filename\n",
    "    \n",
    "        # define embeddings\n",
    "        if model_name == \"text-embedding-ada-002\":\n",
    "            embeddings = OpenAIEmbeddings(model=model_name)\n",
    "        elif model_name != \"BM25\":\n",
    "            embeddings = HuggingFaceBgeEmbeddings(model_name=model_name, model_kwargs = {'device': 'cuda:0'},encode_kwargs = {'normalize_embeddings': True})\n",
    "\n",
    "        # load retriever\n",
    "        if not os.path.exists(index_path):\n",
    "            if chunks == []:\n",
    "                chunks = load_doc_chunks(chunk_size, qa_full_dataset_name)\n",
    "            if model_name == \"BM25\":\n",
    "                retriever = BM25Retriever.from_texts(chunks, metadatas=[{\"source\": 1}] * len(chunks))\n",
    "                retriever.k = TOP_K\n",
    "                save_pk(retriever, index_path)\n",
    "            else:\n",
    "                faiss_vectorstore = FAISS.from_texts(chunks, embeddings)\n",
    "                faiss_vectorstore.save_local(index_path)\n",
    "                retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": TOP_K})\n",
    "        else:\n",
    "            if model_name == \"BM25\":\n",
    "                retriever = load_pk(index_path)\n",
    "                retriever.k = TOP_K\n",
    "            else:\n",
    "                faiss_vectorstore = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)\n",
    "                retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": TOP_K})\n",
    "        retrievers.append(retriever)\n",
    "        \n",
    "    # initialize the ensemble retriever\n",
    "    ensemble_retriever = EnsembleRetriever(\n",
    "        retrievers=retrievers, weights=[1/len(retrievers) for _ in retrievers]\n",
    "    )\n",
    "    return ensemble_retriever\n",
    "\n",
    "def get_response_and_evaluation(data_loaded, knowledge_base):\n",
    "    \n",
    "    correctness = []\n",
    "    results = []\n",
    "    for item in tqdm(data_loaded[:]):\n",
    "\n",
    "        try:\n",
    "            pdf = item[\"filename\"]\n",
    "            ques= [item[\"question_1\"] , item[\"question_2\"], item[\"question_3\"]]\n",
    "            anss = [item[\"answer_1\"], item[\"answer_2\"], item[\"answer_3\"]]\n",
    "        except:\n",
    "            print(\"Key error, please check. \", item)\n",
    "            continue\n",
    "        \n",
    "        for query, answer in zip(ques, anss):\n",
    "            \n",
    "            # Hyde\n",
    "            response_for_retrieval = gpt_llm(Hyde_sys_prompt, query)\n",
    "            docs = knowledge_base.invoke(response_for_retrieval)\n",
    "\n",
    "            QA_prompt = QA_user_prompt.format(\"\\n\".join([docs[i].page_content for i in range(len(docs))]), query)\n",
    "            response = gpt_llm(QA_sys_prompt, QA_prompt)\n",
    "            verified_output, verified_bool = response_evaluation(query, answer, response)\n",
    "            \n",
    "            print(\"======================\")\n",
    "            print(f\"Query: {query}\")\n",
    "            print(f\"Ans: {answer}\")\n",
    "            print(f\"Res: {response}\")\n",
    "            print(f\"Correct or not: {verified_bool}\")\n",
    "            correctness.append(verified_bool)\n",
    "            results.append([pdf, query, answer, response, verified_output, verified_bool])\n",
    "            \n",
    "    return correctness, results\n",
    "\n",
    "def response_evaluation(query, answer, response):\n",
    "    verifier_prompt = verifier_user_prompt.format(query, answer, response)\n",
    "    verified_output = gpt_llm(verifier_sys_prompt, verifier_prompt)\n",
    "    print(\"verified_output: \", verified_output)\n",
    "    verified_bool = verified_output.split('\"answer\": ')[-1]\n",
    "    if \"True\" in verified_bool:\n",
    "        verified_bool = 1\n",
    "    else:\n",
    "        verified_bool = 0\n",
    "    return verified_output, verified_bool\n",
    "\n",
    "def post_response_evaluation(df):\n",
    "    \n",
    "    responses = df.loc[:, \"response\"].values\n",
    "    verified_output = df.loc[:, \"verified_output\"].values\n",
    "    \n",
    "    judged_res = []\n",
    "    for res, v_o in zip(responses, verified_output):\n",
    "        verified_bool = v_o.split('\"answer\": ')[-1]\n",
    "        if \"True\" in verified_bool:\n",
    "            verified_bool = 1\n",
    "        else:\n",
    "            verified_bool = 0\n",
    "        if \"Sorry\" in res:\n",
    "            verified_bool = 0\n",
    "        judged_res.append(verified_bool)\n",
    "    return judged_res\n",
    "\n",
    "model_names = [\"BM25\", \"mixedbread-ai/mxbai-embed-large-v1\", \"BAAI/bge-large-en-v1.5\"]\n",
    "chunk_sizes = [500, 1000, 1500]\n",
    "\n",
    "os.makedirs(\"./results\", exist_ok=True)\n",
    "os.makedirs(\"./faiss\", exist_ok=True)\n",
    "os.makedirs(\"./BM25\", exist_ok=True)\n",
    "os.makedirs(\"./chunks\", exist_ok=True)\n",
    "\n",
    "acc_scores = []\n",
    "for chunk_size in chunk_sizes:\n",
    "    \n",
    "    n = \"fused_hybe\"\n",
    "    result_csv = f\"./results/qa_full-v2_cs-{chunk_size}_{n}.xlsx\"\n",
    "    if os.path.exists(result_csv):\n",
    "        df = pd.read_excel(result_csv)\n",
    "        correctness = df.loc[:,\"verified_bool\"].values\n",
    "        \n",
    "        judged_res = post_response_evaluation(df)\n",
    "        acc_scores.append(np.mean(judged_res))\n",
    "        continue\n",
    "    \n",
    "    ## Create or load kb\n",
    "    knowledge_base = kb_initialization(model_names, chunk_size)\n",
    "    \n",
    "    ## Get output\n",
    "    # load qa dataset\n",
    "    json_filename = \"QA_dataset_v2.json\"\n",
    "    with open(json_filename, 'r') as file:\n",
    "        data_loaded = json.load(file)\n",
    "\n",
    "    ## Evaluation\n",
    "    correctness, results = get_response_and_evaluation(data_loaded, knowledge_base)\n",
    "        \n",
    "    ## Save results to csv file\n",
    "    df = pd.DataFrame(results, columns = [\"filename\", \"query\", \"answer\", \"response\", \"verified_output\", \"verified_bool\"])\n",
    "    df.to_excel(result_csv, index=False)\n",
    "    \n",
    "    ## Re-calculate the accuracy\n",
    "    judged_res = post_response_evaluation(df)\n",
    "    acc_scores.append(np.mean(judged_res))\n",
    "\n",
    "df_res = pd.DataFrame([acc_scores], index=model_names, columns=chunk_sizes)\n",
    "print(df_res)     \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\Software\\Anaconda\\envs\\qa-bot\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "g:\\Software\\Anaconda\\envs\\qa-bot\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "g:\\Software\\Anaconda\\envs\\qa-bot\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verified_output:  {\n",
      "\"reason\": \"The predicted answer's revenue figures for both years do not match the ground truth. The ground truth states Huawei's revenue was 642,338 million CNY in 2021 and 704,174 million CNY in 2022, whereas the predicted answer states the revenue was 636,807 million CNY in 2021 and 642,338 million CNY in 2022.\",\n",
      "\"answer\": False\n",
      "}\n",
      "======================\n",
      "Query: What was Huawei's revenue in CNY for the years 2021 and 2022?\n",
      "Ans: 642,338 million CNY in 2021 and 704,174 million CNY in 2022\n",
      "Res: Huawei's revenue in 2021 was CNY636,807 million and in 2022 was CNY642,338 million.\n",
      "Correct or not: 0\n",
      "verified_output:  {\n",
      "\"reason\": \"The predicted answer matches the ground truth information, stating that Huawei invested more than 20% of its annual sales revenue into R&D over the past three years by the end of 2023.\",\n",
      "\"answer\": True\n",
      "}\n",
      "======================\n",
      "Query: How much did Huawei invest in R&D as a percentage of its annual sales revenue over the past three years by the end of 2023?\n",
      "Ans: More than 20%\n",
      "Res: Over the past three years by the end of 2023, Huawei invested more than 20% of its annual sales revenue into R&D.\n",
      "Correct or not: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:35<01:45, 35.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verified_output:  {\n",
      "\"reason\": \"The predicted answer matches the ground truth information exactly, stating that HarmonyOS had been deployed on more than 800 million devices by the end of 2023.\",\n",
      "\"answer\": True\n",
      "}\n",
      "======================\n",
      "Query: By the end of 2023, how many devices had HarmonyOS been deployed on?\n",
      "Ans: More than 800 million devices\n",
      "Res: By the end of 2023, HarmonyOS had been deployed on more than 800 million devices.\n",
      "Correct or not: 1\n",
      "verified_output:  {\n",
      "\"reason\": \"The key information predicted answer (9%) is different from the ground truth answer (Nearly 11%).\",\n",
      "\"answer\": False\n",
      "}\n",
      "======================\n",
      "Query: What was McDonald's global comparable sales growth rate in 2023 compared to 2022?\n",
      "Ans: Nearly 11%\n",
      "Res: McDonald's global comparable sales growth rate in 2023 was 9% compared to 2022.\n",
      "Correct or not: 0\n",
      "verified_output:  {\n",
      "\"reason\": \"The predicted answer is incorrect because it states that McDonald's systemwide sales increased by nearly $30 billion since 2020 by the end of 2022, while the ground truth is that the increase was nearly $20 billion.\",\n",
      "\"answer\": False\n",
      "}\n",
      "======================\n",
      "Query: By how much did McDonald's systemwide sales increase since 2020 by the end of 2022?\n",
      "Ans: Nearly $20 billion\n",
      "Res: McDonald's Systemwide sales increased by nearly $30 billion since 2020 by the end of 2022.\n",
      "Correct or not: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:53<00:50, 25.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verified_output:  {\n",
      "\"reason\": \"The predicted answer matches the ground truth answer, stating that McDonald's had almost 50 million active loyalty users in its top six markets by the end of 2022.\",\n",
      "\"answer\": True\n",
      "}\n",
      "======================\n",
      "Query: How many active loyalty users did McDonald’s have in its top six markets by the end of 2022?\n",
      "Ans: Almost 50 million\n",
      "Res: McDonald's had almost 50 million active loyalty users in its top six markets by the end of 2022.\n",
      "Correct or not: 1\n",
      "verified_output:  {\n",
      "\"reason\": \"The predicted answer matches the ground truth in terms of the net profit figures for both 2021 and 2022, and it correctly states the increase in net profit from 2021 to 2022. The slight difference in the 2021 figure ($4.86 billion vs. S$4,858 million) is due to rounding but does not change the factual accuracy of the response.\",\n",
      "\"answer\": True\n",
      "}\n",
      "======================\n",
      "Query: What was Oversea-Chinese Banking Corporation Limited's net profit in 2021 and how did it change in 2022?\n",
      "Ans: In 2021, the net profit was S$4,858 million, and it rose to S$5.75 billion in 2022.\n",
      "Res: Oversea-Chinese Banking Corporation Limited's net profit in 2021 was $4.86 billion, and it increased by 18% to $5.75 billion in 2022.\n",
      "Correct or not: 1\n",
      "verified_output:  {\n",
      "\"reason\": \"The predicted answer matches the ground truth in terms of the amounts pledged and achieved by Oversea-Chinese Banking Corporation Limited for sustainable financing commitments by 2025 and the total commitment amount reached by 2022. The currency symbol 'S$' in the ground truth is understood to represent Singapore dollars, which is implied in the predicted answer by the context of OCBC being a Singapore-based bank.\",\n",
      "\"answer\": True\n",
      "}\n",
      "======================\n",
      "Query: How much did Oversea-Chinese Banking Corporation Limited pledge towards achieving its sustainable financing commitments by 2025, and what was the total commitment amount reached by 2022?\n",
      "Ans: OCBC pledged to reach S$50 billion in sustainable finance commitments by 2025 and had reached S$44 billion by 2022.\n",
      "Res: Oversea-Chinese Banking Corporation Limited pledged to achieve $50 billion in sustainable financing commitments by 2025 and reached a total commitment amount of $44 billion by 2022.\n",
      "Correct or not: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [01:16<00:24, 24.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verified_output:  {\n",
      "\"reason\": \"The predicted answer indicates an inability to provide specific details requested in the question, which aligns with the protocol for cases where accurate information cannot be found. Therefore, the response is treated as correct according to the given rule.\",\n",
      "\"answer\": True\n",
      "}\n",
      "======================\n",
      "Query: What was the geographical area featured on OCBC's 2021 annual report cover, and what special project did OCBC launch there as part of its sustainability efforts according to the 2022 report?\n",
      "Ans: The 2021 report featured Guilin, located in Guangxi Province, China, and as part of its sustainability efforts, OCBC gifted two mangrove projects in 2022, including 9,000 trees at the OCBC Mangrove Park in Singapore.\n",
      "Res: Sorry, I do not have an accurate answer for this.\n",
      "Correct or not: 1\n",
      "verified_output:  {\n",
      "\"reason\": \"The predicted answer closely matches the ground truth with specific figures provided for both years, which align with the approximate values given in the ground truth.\", \n",
      "\"answer\": True\n",
      "}\n",
      "======================\n",
      "Query: What was Shell plc's income in 2023 compared to 2022?\n",
      "Ans: Around $20 billion in 2023 compared to around $43 billion in 2022\n",
      "Res: Shell plc's income in 2023 was $19,359 million, compared to $42,309 million in 2022.\n",
      "Correct or not: 1\n",
      "verified_output:  {\n",
      "\"reason\": \"The predicted answer matches the ground truth answer exactly, stating that Shell plc returned $23 billion to shareholders through share buybacks and dividends in 2023.\",\n",
      "\"answer\": True\n",
      "}\n",
      "======================\n",
      "Query: How much did Shell plc return to shareholders in 2023 through share buybacks and dividends?\n",
      "Ans: $23 billion\n",
      "Res: In 2023, Shell plc returned $23 billion to shareholders through share buybacks and dividends.\n",
      "Correct or not: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:37<00:00, 24.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verified_output:  {\n",
      "\"reason\": \"The predicted answer matches the ground truth answer exactly, stating the planned investment amount by Shell plc in oil and gas between 2023 and 2025 as around $40 billion.\",\n",
      "\"answer\": True\n",
      "}\n",
      "======================\n",
      "Query: How much did Shell plc plan to invest in oil and gas between 2023 and 2025?\n",
      "Ans: Around $40 billion\n",
      "Res: Shell plc planned to invest around $40 billion in oil and gas between 2023 and 2025.\n",
      "Correct or not: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "g:\\Software\\Anaconda\\envs\\qa-bot\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verified_output:  {\n",
      "\"reason\": \"The predicted answer's revenue figures for both 2021 and 2022 do not match the ground truth. The ground truth states Huawei's revenue was 642,338 million CNY in 2021 and 704,174 million CNY in 2022, whereas the predicted answer states Huawei's revenue was 636,807 million CNY in 2021 and 642,338 million CNY in 2022.\",\n",
      "\"answer\": False\n",
      "}\n",
      "======================\n",
      "Query: What was Huawei's revenue in CNY for the years 2021 and 2022?\n",
      "Ans: 642,338 million CNY in 2021 and 704,174 million CNY in 2022\n",
      "Res: Huawei's revenue for 2021 was CNY 636,807 million and for 2022 was CNY 642,338 million.\n",
      "Correct or not: 0\n",
      "verified_output:  {\n",
      "\"reason\": \"The predicted answer matches the ground truth information. Both state that Huawei invested more than 20% of its annual sales revenue into R&D each year over the past three years by the end of 2023.\", \n",
      "\"answer\": True\n",
      "}\n",
      "======================\n",
      "Query: How much did Huawei invest in R&D as a percentage of its annual sales revenue over the past three years by the end of 2023?\n",
      "Ans: More than 20%\n",
      "Res: Over the past three years by the end of 2023, Huawei invested more than 20% of its annual sales revenue into R&D each year.\n",
      "Correct or not: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:25<01:15, 25.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verified_output:  {\n",
      "\"reason\": \"The predicted answer matches the ground truth information exactly, stating that HarmonyOS had been deployed on more than 800 million devices by the end of 2023.\",\n",
      "\"answer\": True\n",
      "}\n",
      "======================\n",
      "Query: By the end of 2023, how many devices had HarmonyOS been deployed on?\n",
      "Ans: More than 800 million devices\n",
      "Res: By the end of 2023, HarmonyOS had been deployed on more than 800 million devices.\n",
      "Correct or not: 1\n",
      "verified_output:  {\n",
      "\"reason\": \"The key information predicted answer (9%) is different from the ground truth answer (Nearly 11%).\",\n",
      "\"answer\": False\n",
      "}\n",
      "======================\n",
      "Query: What was McDonald's global comparable sales growth rate in 2023 compared to 2022?\n",
      "Ans: Nearly 11%\n",
      "Res: McDonald's global comparable sales grew by 9% in 2023 compared to 2022.\n",
      "Correct or not: 0\n",
      "verified_output:  {\n",
      "\"reason\": \"Since the predicted answer indicates an inability to provide a specific figure, and it does not contradict the ground truth information, it can be treated as correct because it acknowledges the lack of precise data available.\",\n",
      "\"answer\": True\n",
      "}\n",
      "======================\n",
      "Query: By how much did McDonald's systemwide sales increase since 2020 by the end of 2022?\n",
      "Ans: Nearly $20 billion\n",
      "Res: Sorry, I do not have an accurate answer for this.\n",
      "Correct or not: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:53<00:54, 27.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verified_output:  {\n",
      "\"reason\": \"The predicted answer matches the ground truth answer, stating that McDonald's had almost 50 million active loyalty users in its top six markets by the end of 2022.\",\n",
      "\"answer\": True\n",
      "}\n",
      "======================\n",
      "Query: How many active loyalty users did McDonald’s have in its top six markets by the end of 2022?\n",
      "Ans: Almost 50 million\n",
      "Res: McDonald's had almost 50 million active loyalty users in its top six markets by the end of 2022.\n",
      "Correct or not: 1\n",
      "verified_output:  {\n",
      "\"reason\": \"The predicted answer matches the ground truth in terms of the net profit figures for both 2021 and 2022, with a minor rounding difference for the 2021 figure ($4.86 billion vs. S$4,858 million). The change in net profit from 2021 to 2022 is correctly described as an increase.\",\n",
      "\"answer\": True\n",
      "}\n",
      "======================\n",
      "Query: What was Oversea-Chinese Banking Corporation Limited's net profit in 2021 and how did it change in 2022?\n",
      "Ans: In 2021, the net profit was S$4,858 million, and it rose to S$5.75 billion in 2022.\n",
      "Res: Oversea-Chinese Banking Corporation Limited's net profit in 2021 was $4.86 billion, and it increased to $5.75 billion in 2022.\n",
      "Correct or not: 1\n",
      "verified_output:  {\n",
      "\"reason\": \"The predicted answer matches the ground truth in terms of the amounts pledged and achieved by Oversea-Chinese Banking Corporation Limited for sustainable financing commitments by 2025 and the total commitment amount reached by 2022. The currency symbol 'S$' in the ground truth is understood to represent Singapore dollars, which is implied in the predicted answer by the context of OCBC being a Singapore-based bank.\",\n",
      "\"answer\": True\n",
      "}\n",
      "======================\n",
      "Query: How much did Oversea-Chinese Banking Corporation Limited pledge towards achieving its sustainable financing commitments by 2025, and what was the total commitment amount reached by 2022?\n",
      "Ans: OCBC pledged to reach S$50 billion in sustainable finance commitments by 2025 and had reached S$44 billion by 2022.\n",
      "Res: Oversea-Chinese Banking Corporation Limited pledged to achieve $50 billion in sustainable financing commitments by 2025 and reached a total commitment amount of $44 billion by 2022.\n",
      "Correct or not: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [01:21<00:27, 27.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verified_output:  {\n",
      "\"reason\": \"The predicted answer incorrectly states that the geographical area featured on OCBC's 2021 annual report cover is not specified, whereas the ground truth specifies it as Guilin, located in Guangxi Province, China. Additionally, the predicted answer does not mention the gifting of two mangrove projects in 2022, including 9,000 trees at the OCBC Mangrove Park in Singapore, which is a key detail in the ground truth.\", \n",
      "\"answer\": False\n",
      "}\n",
      "======================\n",
      "Query: What was the geographical area featured on OCBC's 2021 annual report cover, and what special project did OCBC launch there as part of its sustainability efforts according to the 2022 report?\n",
      "Ans: The 2021 report featured Guilin, located in Guangxi Province, China, and as part of its sustainability efforts, OCBC gifted two mangrove projects in 2022, including 9,000 trees at the OCBC Mangrove Park in Singapore.\n",
      "Res: The geographical area featured on OCBC's 2021 annual report cover is not specified in the provided information. However, according to the 2022 report, OCBC launched a large-scale ecological mangrove conservation project in partnership with NParks in Singapore as part of its sustainability efforts.\n",
      "Correct or not: 0\n",
      "verified_output:  {\n",
      "\"reason\": \"The predicted answer closely matches the ground truth, providing specific figures that align with the approximate amounts given in the ground truth. The slight difference in the figures is due to the specificity of the predicted answer, which provides exact numbers rather than rounded figures.\",\n",
      "\"answer\": True\n",
      "}\n",
      "======================\n",
      "Query: What was Shell plc's income in 2023 compared to 2022?\n",
      "Ans: Around $20 billion in 2023 compared to around $43 billion in 2022\n",
      "Res: Shell plc's income in 2023 was $19,359 million, compared with $42,309 million in 2022.\n",
      "Correct or not: 1\n",
      "verified_output:  {\n",
      "\"reason\": \"The predicted answer matches the ground truth answer, stating that Shell plc returned $23 billion to shareholders through share buybacks and dividends in 2023.\",\n",
      "\"answer\": True\n",
      "}\n",
      "======================\n",
      "Query: How much did Shell plc return to shareholders in 2023 through share buybacks and dividends?\n",
      "Ans: $23 billion\n",
      "Res: In 2023, Shell plc returned $23 billion to shareholders through share buybacks and dividends.\n",
      "Correct or not: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:40<00:00, 25.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verified_output:  {\n",
      "\"reason\": \"The predicted answer matches the ground truth answer exactly, stating the planned investment amount by Shell plc in oil and gas between 2023 and 2025.\",\n",
      "\"answer\": True\n",
      "}\n",
      "======================\n",
      "Query: How much did Shell plc plan to invest in oil and gas between 2023 and 2025?\n",
      "Ans: Around $40 billion\n",
      "Res: Shell plc planned to invest around $40 billion in oil and gas between 2023 and 2025.\n",
      "Correct or not: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "g:\\Software\\Anaconda\\envs\\qa-bot\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verified_output:  {\n",
      "\"reason\": \"The predicted answer's revenue figures for both 2021 and 2022 do not match the ground truth data. The ground truth states Huawei's revenue was 642,338 million CNY in 2021 and 704,174 million CNY in 2022, whereas the predicted answer provides different figures for both years.\",\n",
      "\"answer\": False\n",
      "}\n",
      "======================\n",
      "Query: What was Huawei's revenue in CNY for the years 2021 and 2022?\n",
      "Ans: 642,338 million CNY in 2021 and 704,174 million CNY in 2022\n",
      "Res: Huawei's revenue for 2021 was CNY 636,807 million and for 2022 was CNY 642,338 million.\n",
      "Correct or not: 0\n",
      "verified_output:  {\n",
      "\"reason\": \"The predicted answer provides specific percentages of Huawei's R&D investment relative to its annual sales revenue for each of the past three years, all of which are more than 20%. This directly supports the ground truth claim that Huawei invested more than 20% of its annual sales revenue in R&D over the past three years by the end of 2023.\", \n",
      "\"answer\": True\n",
      "}\n",
      "======================\n",
      "Query: How much did Huawei invest in R&D as a percentage of its annual sales revenue over the past three years by the end of 2023?\n",
      "Ans: More than 20%\n",
      "Res: Over the past three years by the end of 2023, Huawei invested the following percentages of its annual sales revenue into R&D:\n",
      "\n",
      "- 2021: 22.4%\n",
      "- 2022: 25.1%\n",
      "- 2023: 23.4%\n",
      "Correct or not: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:27<01:22, 27.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verified_output:  {\n",
      "\"reason\": \"The predicted answer matches the ground truth answer exactly, stating that HarmonyOS had been deployed on more than 800 million devices by the end of 2023.\",\n",
      "\"answer\": True\n",
      "}\n",
      "======================\n",
      "Query: By the end of 2023, how many devices had HarmonyOS been deployed on?\n",
      "Ans: More than 800 million devices\n",
      "Res: By the end of 2023, HarmonyOS had been deployed on more than 800 million devices.\n",
      "Correct or not: 1\n",
      "verified_output:  {\n",
      "\"reason\": \"The key information in the predicted answer does not match the ground truth. The ground truth states a nearly 11% growth rate, whereas the predicted answer states a 9% growth rate.\",\n",
      "\"answer\": False\n",
      "}\n",
      "======================\n",
      "Query: What was McDonald's global comparable sales growth rate in 2023 compared to 2022?\n",
      "Ans: Nearly 11%\n",
      "Res: McDonald's global comparable sales grew by 9% in 2023 compared to 2022.\n",
      "Correct or not: 0\n",
      "verified_output:  {\n",
      "\"reason\": \"The predicted answer is incorrect because it states that McDonald's systemwide sales increased by nearly $30 billion, while the ground truth is that the increase was nearly $20 billion.\",\n",
      "\"answer\": False\n",
      "}\n",
      "======================\n",
      "Query: By how much did McDonald's systemwide sales increase since 2020 by the end of 2022?\n",
      "Ans: Nearly $20 billion\n",
      "Res: McDonald's Systemwide sales increased by nearly $30 billion since 2020 by the end of 2022.\n",
      "Correct or not: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:58<00:58, 29.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verified_output:  {\n",
      "\"reason\": \"The predicted answer matches the ground truth information, stating that McDonald's had almost 50 million active loyalty users in its top six markets by the end of 2022.\",\n",
      "\"answer\": True\n",
      "}\n",
      "======================\n",
      "Query: How many active loyalty users did McDonald’s have in its top six markets by the end of 2022?\n",
      "Ans: Almost 50 million\n",
      "Res: By the end of 2022, McDonald's had almost 50 million active loyalty users in its top six markets.\n",
      "Correct or not: 1\n",
      "verified_output:  {\n",
      "\"reason\": \"The predicted answer matches the ground truth in terms of the net profit figures for both 2021 and 2022, with a minor rounding difference for the 2021 figure (S$4,858 million vs. $4.86 billion). The change in net profit from 2021 to 2022 is correctly described as an increase.\",\n",
      "\"answer\": True\n",
      "}\n",
      "======================\n",
      "Query: What was Oversea-Chinese Banking Corporation Limited's net profit in 2021 and how did it change in 2022?\n",
      "Ans: In 2021, the net profit was S$4,858 million, and it rose to S$5.75 billion in 2022.\n",
      "Res: Oversea-Chinese Banking Corporation Limited's net profit in 2021 was $4.86 billion, and it increased to $5.75 billion in 2022.\n",
      "Correct or not: 1\n",
      "verified_output:  {\n",
      "\"reason\": \"The predicted answer matches the ground truth in terms of the amounts pledged and achieved by Oversea-Chinese Banking Corporation Limited for its sustainable financing commitments by 2025 and the total commitment amount reached by 2022, respectively. The currency symbol 'S$' in the ground truth and '$' in the predicted answer both refer to Singapore dollars, which is the contextually correct currency for OCBC, a Singapore-based bank.\", \n",
      "\"answer\": True\n",
      "}\n",
      "======================\n",
      "Query: How much did Oversea-Chinese Banking Corporation Limited pledge towards achieving its sustainable financing commitments by 2025, and what was the total commitment amount reached by 2022?\n",
      "Ans: OCBC pledged to reach S$50 billion in sustainable finance commitments by 2025 and had reached S$44 billion by 2022.\n",
      "Res: Oversea-Chinese Banking Corporation Limited pledged to achieve $50 billion in sustainable financing commitments by 2025 and reached a total commitment amount of $44 billion by 2022.\n",
      "Correct or not: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [01:30<00:30, 30.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verified_output:  {\n",
      "\"reason\": \"The predicted answer incorrectly identifies the geographical area featured on OCBC's 2021 annual report cover as Pulau Ubin, Singapore, while the correct location is Guilin, located in Guangxi Province, China. Additionally, the special project mentioned in the prediction, the OCBC Mangrove Park project, is correctly associated with OCBC's sustainability efforts but incorrectly linked to Pulau Ubin instead of being part of the broader initiative that included gifting two mangrove projects in 2022, including 9,000 trees at the OCBC Mangrove Park in Singapore.\",\n",
      "\"answer\": False\n",
      "}\n",
      "======================\n",
      "Query: What was the geographical area featured on OCBC's 2021 annual report cover, and what special project did OCBC launch there as part of its sustainability efforts according to the 2022 report?\n",
      "Ans: The 2021 report featured Guilin, located in Guangxi Province, China, and as part of its sustainability efforts, OCBC gifted two mangrove projects in 2022, including 9,000 trees at the OCBC Mangrove Park in Singapore.\n",
      "Res: The geographical area featured on OCBC's 2021 annual report cover was Pulau Ubin, Singapore. As part of its sustainability efforts, OCBC launched the OCBC Mangrove Park project there according to the 2022 report.\n",
      "Correct or not: 0\n",
      "verified_output:  {\n",
      "\"reason\": \"The predicted answer closely matches the ground truth in terms of the income figures for Shell plc in 2023 and 2022, with slight differences due to rounding. The predicted answer provides specific figures that align with the ground truth's 'around $20 billion in 2023' and 'around $43 billion in 2022'.\",\n",
      "\"answer\": True\n",
      "}\n",
      "======================\n",
      "Query: What was Shell plc's income in 2023 compared to 2022?\n",
      "Ans: Around $20 billion in 2023 compared to around $43 billion in 2022\n",
      "Res: Shell plc's income in 2023 was $19,636 million, compared with $42,874 million in 2022.\n",
      "Correct or not: 1\n",
      "verified_output:  {\n",
      "\"reason\": \"The predicted answer matches the ground truth answer, stating that Shell plc returned approximately $23 billion to shareholders through share buybacks and dividends in 2023.\",\n",
      "\"answer\": True\n",
      "}\n",
      "======================\n",
      "Query: How much did Shell plc return to shareholders in 2023 through share buybacks and dividends?\n",
      "Ans: $23 billion\n",
      "Res: In 2023, Shell plc returned approximately $23 billion to shareholders through share buybacks and dividends.\n",
      "Correct or not: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:54<00:00, 28.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verified_output:  {\n",
      "\"reason\": \"The predicted answer matches the ground truth answer exactly, stating the planned investment amount by Shell plc in oil and gas between 2023 and 2025.\",\n",
      "\"answer\": True\n",
      "}\n",
      "======================\n",
      "Query: How much did Shell plc plan to invest in oil and gas between 2023 and 2025?\n",
      "Ans: Around $40 billion\n",
      "Res: Shell plc planned to invest around $40 billion in oil and gas between 2023 and 2025.\n",
      "Correct or not: 1\n",
      "                                        500       1000      1500\n",
      "BM25                                0.666667  0.666667  0.666667\n",
      "mixedbread-ai/mxbai-embed-large-v1  0.666667  0.666667  0.666667\n",
      "BAAI/bge-large-en-v1.5              0.666667  0.666667  0.666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "import pandas as pd\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "import pickle\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())  # read local .env file\n",
    "qa_full_dataset_name = \"QA_dataset_v2\"  # Define dataset name\n",
    "TOP_K = 5\n",
    "\n",
    "def save_pk(chunks, file_path):\n",
    "    with open(file_path, \"wb\") as file:\n",
    "        pickle.dump(chunks, file)\n",
    "\n",
    "def load_pk(file_path):\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "def load_doc_chunks(chunk_size, qa_full_dataset_name):\n",
    "    \n",
    "    chunks = []\n",
    "    file_path = f\"./chunks/chunks_{chunk_size}_{qa_full_dataset_name}.pkl\" \n",
    "    \n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_size//5,\n",
    "        length_function=len\n",
    "    )\n",
    "    try:\n",
    "        # Check if the chunks file already exists\n",
    "        chunks = load_pk(file_path)\n",
    "        print(\"Loaded chunks from existing file. \", file_path)\n",
    "    except FileNotFoundError:\n",
    "        # If the file does not exist, process the PDFs\n",
    "        chunks = []\n",
    "        for pdf in tqdm(glob.glob(\"./docs/*.pdf\")):\n",
    "            pdf_reader = PdfReader(pdf)\n",
    "            text = \"\"\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text() if page.extract_text() else \"\"\n",
    "            chunks += text_splitter.split_text(text)\n",
    "        \n",
    "        # Save the chunks to a file after processing\n",
    "        save_pk(chunks, file_path)\n",
    "        print(\"Saved new chunks to file. \", file_path)\n",
    "    return chunks\n",
    "        \n",
    "def kb_initialization(model_names, chunk_size):\n",
    "    \n",
    "    chunks = []   \n",
    "    retrievers = []\n",
    "    for model_name in model_names:\n",
    "        \n",
    "        # define retriever saving path\n",
    "        index_filename = f\"faiss_index_cs-{chunk_size}_\" + model_name.split(\"/\")[-1]\n",
    "        index_path = \"./faiss/\" + index_filename\n",
    "    \n",
    "        # define embeddings\n",
    "        if model_name == \"text-embedding-ada-002\":\n",
    "            embeddings = OpenAIEmbeddings(model=model_name)\n",
    "        elif model_name != \"BM25\":\n",
    "            embeddings = HuggingFaceBgeEmbeddings(model_name=model_name, model_kwargs = {'device': 'cuda:0'},encode_kwargs = {'normalize_embeddings': True})\n",
    "\n",
    "        # load retriever\n",
    "        if not os.path.exists(index_path):\n",
    "            if chunks == []:\n",
    "                chunks = load_doc_chunks(chunk_size, qa_full_dataset_name)\n",
    "            if model_name == \"BM25\":\n",
    "                retriever = BM25Retriever.from_texts(chunks, metadatas=[{\"source\": 1}] * len(chunks))\n",
    "                retriever.k = TOP_K\n",
    "                save_pk(retriever, index_path)\n",
    "            else:\n",
    "                faiss_vectorstore = FAISS.from_texts(chunks, embeddings)\n",
    "                faiss_vectorstore.save_local(index_path)\n",
    "                retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": TOP_K})\n",
    "        else:\n",
    "            if model_name == \"BM25\":\n",
    "                retriever = load_pk(index_path)\n",
    "                retriever.k = TOP_K\n",
    "            else:\n",
    "                faiss_vectorstore = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)\n",
    "                retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": TOP_K})\n",
    "        retrievers.append(retriever)\n",
    "        \n",
    "    # initialize the ensemble retriever\n",
    "    ensemble_retriever = EnsembleRetriever(\n",
    "        retrievers=retrievers, weights=[1/len(retrievers) for _ in retrievers]\n",
    "    )\n",
    "    return ensemble_retriever\n",
    "\n",
    "def get_response_and_evaluation(data_loaded, knowledge_base):\n",
    "    \n",
    "    correctness = []\n",
    "    results = []\n",
    "    for item in tqdm(data_loaded[:]):\n",
    "\n",
    "        try:\n",
    "            pdf = item[\"filename\"]\n",
    "            ques= [item[\"question_1\"] , item[\"question_2\"], item[\"question_3\"]]\n",
    "            anss = [item[\"answer_1\"], item[\"answer_2\"], item[\"answer_3\"]]\n",
    "        except:\n",
    "            print(\"Key error, please check. \", item)\n",
    "            continue\n",
    "        \n",
    "        for query, answer in zip(ques, anss):\n",
    "            \n",
    "            # Hyde\n",
    "            response_for_retrieval = gpt_llm(Hyde_sys_prompt, query)\n",
    "            docs = knowledge_base.invoke(response_for_retrieval)\n",
    "\n",
    "            QA_prompt = QA_user_prompt.format(\"\\n\".join([docs[i].page_content for i in range(len(docs))]), query)\n",
    "            response = gpt_llm(QA_sys_prompt, QA_prompt)\n",
    "            verified_output, verified_bool = response_evaluation(query, answer, response)\n",
    "            \n",
    "            print(\"======================\")\n",
    "            print(f\"Query: {query}\")\n",
    "            print(f\"Ans: {answer}\")\n",
    "            print(f\"Res: {response}\")\n",
    "            print(f\"Correct or not: {verified_bool}\")\n",
    "            correctness.append(verified_bool)\n",
    "            results.append([pdf, query, answer, response, verified_output, verified_bool])\n",
    "            \n",
    "    return correctness, results\n",
    "\n",
    "def response_evaluation(query, answer, response):\n",
    "    verifier_prompt = verifier_user_prompt.format(query, answer, response)\n",
    "    verified_output = gpt_llm(verifier_sys_prompt, verifier_prompt)\n",
    "    print(\"verified_output: \", verified_output)\n",
    "    verified_bool = verified_output.split('\"answer\": ')[-1]\n",
    "    if \"True\" in verified_bool:\n",
    "        verified_bool = 1\n",
    "    else:\n",
    "        verified_bool = 0\n",
    "    return verified_output, verified_bool\n",
    "\n",
    "def post_response_evaluation(df):\n",
    "    \n",
    "    responses = df.loc[:, \"response\"].values\n",
    "    verified_output = df.loc[:, \"verified_output\"].values\n",
    "    \n",
    "    judged_res = []\n",
    "    for res, v_o in zip(responses, verified_output):\n",
    "        verified_bool = v_o.split('\"answer\": ')[-1]\n",
    "        if \"True\" in verified_bool:\n",
    "            verified_bool = 1\n",
    "        else:\n",
    "            verified_bool = 0\n",
    "        if \"Sorry\" in res:\n",
    "            verified_bool = 0\n",
    "        judged_res.append(verified_bool)\n",
    "    return judged_res\n",
    "\n",
    "model_names = [\"BM25\", \"mixedbread-ai/mxbai-embed-large-v1\", \"BAAI/bge-large-en-v1.5\"]\n",
    "chunk_sizes = [500, 1000, 1500]\n",
    "\n",
    "os.makedirs(\"./results\", exist_ok=True)\n",
    "os.makedirs(\"./faiss\", exist_ok=True)\n",
    "os.makedirs(\"./BM25\", exist_ok=True)\n",
    "os.makedirs(\"./chunks\", exist_ok=True)\n",
    "\n",
    "acc_scores = []\n",
    "for chunk_size in chunk_sizes:\n",
    "    \n",
    "    n = \"fused_hybe\"\n",
    "    result_csv = f\"./results/qa_full-v3_cs-{chunk_size}_{n}.xlsx\"\n",
    "    if os.path.exists(result_csv):\n",
    "        df = pd.read_excel(result_csv)\n",
    "        correctness = df.loc[:,\"verified_bool\"].values\n",
    "        \n",
    "        judged_res = post_response_evaluation(df)\n",
    "        acc_scores.append(np.mean(judged_res))\n",
    "        continue\n",
    "    \n",
    "    ## Create or load kb\n",
    "    knowledge_base = kb_initialization(model_names, chunk_size) \n",
    "    \n",
    "    ## Get output\n",
    "    # load qa dataset\n",
    "    json_filename = \"QA_dataset_v3.json\"\n",
    "    with open(json_filename, 'r') as file:\n",
    "        data_loaded = json.load(file)\n",
    "\n",
    "    ## Evaluation\n",
    "    correctness, results = get_response_and_evaluation(data_loaded, knowledge_base)\n",
    "        \n",
    "    ## Save results to csv file\n",
    "    df = pd.DataFrame(results, columns = [\"filename\", \"query\", \"answer\", \"response\", \"verified_output\", \"verified_bool\"])\n",
    "    df.to_excel(result_csv, index=False)\n",
    "    \n",
    "    ## Re-calculate the accuracy\n",
    "    judged_res = post_response_evaluation(df)\n",
    "    acc_scores.append(np.mean(judged_res))\n",
    "\n",
    "df_res = pd.DataFrame([acc_scores], index=model_names, columns=chunk_sizes)\n",
    "print(df_res)     \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "badminton_court_booking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
