{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import streamlit as st\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "import pandas as pd\n",
    "\n",
    "# load qa dataset\n",
    "# Reading JSON data\n",
    "json_filename = \"QA_dataset.json\"\n",
    "with open(json_filename, 'r') as file:\n",
    "    data_loaded = json.load(file)\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())  # read local .env file\n",
    "\n",
    "def process(item):\n",
    "\n",
    "    try:\n",
    "        pdf = item[\"filename\"]\n",
    "        ques= [item[\"question_1\"] , item[\"question_2\"], item[\"question_3\"]]\n",
    "        anss = [item[\"answer_1\"], item[\"answer_2\"], item[\"answer_3\"]]\n",
    "    except:\n",
    "        print(\"Key error, please check. \", item)\n",
    "        return []\n",
    "    \n",
    "    pdf_reader = PdfReader(pdf)\n",
    "    text = \"\"\n",
    "    for page in pdf_reader.pages:\n",
    "        text += page.extract_text()\n",
    "\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len\n",
    "    )  \n",
    "\n",
    "    chunks = text_splitter.split_text(text)\n",
    "\n",
    "    # embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    embeddings = GPT4AllEmbeddings()\n",
    "    save_faiss_index = False\n",
    "    index_filename = \"faiss_index.gpt4all\"\n",
    "    if save_faiss_index:\n",
    "        if not os.path.exists(index_filename):\n",
    "            knowledge_base = FAISS.from_texts(chunks, embeddings)\n",
    "            knowledge_base.save_local(index_filename)\n",
    "        else:\n",
    "            knowledge_base = FAISS.load_local(index_filename, embeddings, allow_dangerous_deserialization=True)\n",
    "    else:\n",
    "        knowledge_base = FAISS.from_texts(chunks, embeddings)\n",
    "        \n",
    "    result = []\n",
    "    for query, answer in zip(ques, anss):\n",
    "        docs = knowledge_base.similarity_search(query)\n",
    "\n",
    "        llm = OpenAI()\n",
    "        chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "        response = chain.run(input_documents=docs, question=query)\n",
    "\n",
    "        print(\"======================\")\n",
    "        print(f\"Query: {query}\")\n",
    "        print(f\"Ans: {answer}\")\n",
    "        print(f\"Res: {response}\")\n",
    "        \n",
    "        verifier_prompt = \"\"\"## Role: Answer verifier\n",
    "\n",
    "## Goal\n",
    "You can judge whether the answer is correct or not. \n",
    "\n",
    "## Rule\n",
    "- If the key information predicted answer is same as the ground truth answer, then the answer is correct.\n",
    "\n",
    "\n",
    "## Output format\n",
    "{\n",
    "\"reason\": \"fill the reason why the predicted answer is wrong (False) or correct (True).\", \n",
    "\"answer\": True or False\n",
    "}\n",
    "\n",
    "\"\"\" + f\"\"\"\n",
    "\n",
    "The question is: {query}\n",
    "Ground truth is: {answer}\n",
    "Predicted answer is: {response}\n",
    "\"\"\"\n",
    "        verified_output = llm.invoke(verifier_prompt)\n",
    "        print(\"verified_output: \", verified_output)\n",
    "        verified_bool = verified_output.split('\"answer\": ')[-1]\n",
    "        if \"True\" in verified_bool:\n",
    "            verified_bool = 1\n",
    "        else:\n",
    "            verified_bool = 0\n",
    "\n",
    "        result.append([pdf, query, answer, response, verified_bool])\n",
    "\n",
    "    return result\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
    "    results = list(executor.map(process, data_loaded))\n",
    "result = []\n",
    "for x in results:\n",
    "    result += x\n",
    "    \n",
    "# save results\n",
    "os.makedirs(\"./results\", exist_ok=True)\n",
    "result_csv = \"./results/qa_single_cs-1000.csv\"\n",
    "df = pd.DataFrame(result, columns = [\"filename\", \"query\", \"answer\", \"response\", \"verified_bool\"])\n",
    "df.to_csv(result_csv, index=False)\n",
    "\n",
    "# calculate the accuracy\n",
    "correctness = df.loc[:, \"verified_bool\"].values\n",
    "print(correctness)\n",
    "print(np.mean(correctness))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "badminton_court_booking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
