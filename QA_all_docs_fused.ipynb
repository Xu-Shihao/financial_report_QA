{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_sys_prompt = \"\"\"\n",
    "# Character\n",
    "You're a skilled chatbot, capable of extracting relevant information from retrieved documents. When a user poses a question, you answer it as though you have firsthand knowledge rather than referencing a document.\n",
    "\n",
    "## Skills\n",
    "### Skill 1: Answer questions about the retrieved document\n",
    "- Understand the user's question.\n",
    "- Analyze the retrieved document to find relevant information.\n",
    "\n",
    "### Skill 2: Reply when unable to answer\n",
    "- If the question can't be answered based on the document, respond, \"Sorry, I do not have an accurate answer for this.\"\n",
    "\n",
    "## Constraints\n",
    "- Mimic the tone and language used by a chatbot.\n",
    "- Do not reference any document or outside source in your answers.\n",
    "- If no accurate answer can be provided, be honest and inform the user.\n",
    "\"\"\"\n",
    "\n",
    "QA_user_prompt = \"\"\"\n",
    "## Reference document\n",
    "{}\n",
    "\n",
    "Answer this query: {} Make the answer short and clean.\n",
    "\"\"\"\n",
    "\n",
    "verifier_sys_prompt = \"\"\"## Role: Answer verifier\n",
    "\n",
    "## Goal\n",
    "You can judge whether the answer is correct or not. \n",
    "\n",
    "## Rule\n",
    "- If the key information predicted answer is same as the ground truth answer, then the answer is correct.\n",
    "- If the response is \"Sorry, I do not have an accurate answer for this.\", it means the answer can not be found, then the answer can be treated as correct.\n",
    "\n",
    "## Output format\n",
    "{{\n",
    "\"reason\": \"fill the reason why the predicted answer is wrong (False) or correct (True).\", \n",
    "\"answer\": True or False\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "verifier_user_prompt = \"\"\"\n",
    "The question is: {}\n",
    "Ground truth is: {}\n",
    "Predicted answer is: {}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_llm(system_prompt, user_prompt):\n",
    "    from openai import OpenAI\n",
    "\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo-preview\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        temperature=0,\n",
    "        top_p = 0.9\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# gpt_llm(\"You are a helpful assistant.\", \"Who are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           500       1000      1500\n",
      "fused  0.952381  0.952381  0.928571\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "import pandas as pd\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "import pickle\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())  # read local .env file\n",
    "qa_full_dataset_name = \"QA_dataset_v2\"  # Define dataset name\n",
    "TOP_K = 5\n",
    "\n",
    "def save_pk(chunks, file_path):\n",
    "    with open(file_path, \"wb\") as file:\n",
    "        pickle.dump(chunks, file)\n",
    "\n",
    "def load_pk(file_path):\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "def load_doc_chunks(chunk_size, qa_full_dataset_name):\n",
    "    \n",
    "    chunks = []\n",
    "    file_path = f\"./chunks/chunks_{chunk_size}_{qa_full_dataset_name}.pkl\" \n",
    "    \n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_size//5,\n",
    "        length_function=len\n",
    "    )\n",
    "    try:\n",
    "        # Check if the chunks file already exists\n",
    "        chunks = load_pk(file_path)\n",
    "        print(\"Loaded chunks from existing file. \", file_path)\n",
    "    except FileNotFoundError:\n",
    "        # If the file does not exist, process the PDFs\n",
    "        chunks = []\n",
    "        for pdf in tqdm(glob.glob(\"./docs/*.pdf\")):\n",
    "            pdf_reader = PdfReader(pdf)\n",
    "            text = \"\"\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text() if page.extract_text() else \"\"\n",
    "            chunks += text_splitter.split_text(text)\n",
    "        \n",
    "        # Save the chunks to a file after processing\n",
    "        save_pk(chunks, file_path)\n",
    "        print(\"Saved new chunks to file. \", file_path)\n",
    "    return chunks\n",
    "        \n",
    "def kb_initialization(model_names, chunk_size):\n",
    "    \n",
    "    chunks = []   \n",
    "    retrievers = []\n",
    "    for model_name in model_names:\n",
    "        \n",
    "        # define retriever saving path\n",
    "        index_filename = f\"faiss_index_cs-{chunk_size}_\" + model_name.split(\"/\")[-1]\n",
    "        index_path = \"./faiss/\" + index_filename\n",
    "    \n",
    "        # define embeddings\n",
    "        if model_name == \"text-embedding-ada-002\":\n",
    "            embeddings = OpenAIEmbeddings(model=model_name)\n",
    "        elif model_name != \"BM25\":\n",
    "            embeddings = HuggingFaceBgeEmbeddings(model_name=model_name, model_kwargs = {'device': 'cuda:0'},encode_kwargs = {'normalize_embeddings': True})\n",
    "\n",
    "        # load retriever\n",
    "        if not os.path.exists(index_path):\n",
    "            if chunks == []:\n",
    "                chunks = load_doc_chunks(chunk_size, qa_full_dataset_name)\n",
    "            if model_name == \"BM25\":\n",
    "                retriever = BM25Retriever.from_texts(chunks, metadatas=[{\"source\": 1}] * len(chunks))\n",
    "                retriever.k = TOP_K\n",
    "                save_pk(retriever, index_path)\n",
    "            else:\n",
    "                faiss_vectorstore = FAISS.from_texts(chunks, embeddings)\n",
    "                faiss_vectorstore.save_local(index_path)\n",
    "                retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": TOP_K})\n",
    "        else:\n",
    "            if model_name == \"BM25\":\n",
    "                retriever = load_pk(index_path)\n",
    "                retriever.k = TOP_K\n",
    "            else:\n",
    "                faiss_vectorstore = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)\n",
    "                retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": TOP_K})\n",
    "        retrievers.append(retriever)\n",
    "        \n",
    "    # initialize the ensemble retriever\n",
    "    ensemble_retriever = EnsembleRetriever(\n",
    "        retrievers=retrievers, weights=[1/len(retrievers) for _ in retrievers]\n",
    "    )\n",
    "    return ensemble_retriever\n",
    "\n",
    "def get_response_and_evaluation(data_loaded, knowledge_base):\n",
    "    \n",
    "    correctness = []\n",
    "    results = []\n",
    "    for item in tqdm(data_loaded[:]):\n",
    "\n",
    "        try:\n",
    "            pdf = item[\"filename\"]\n",
    "            ques= [item[\"question_1\"] , item[\"question_2\"], item[\"question_3\"]]\n",
    "            anss = [item[\"answer_1\"], item[\"answer_2\"], item[\"answer_3\"]]\n",
    "        except:\n",
    "            print(\"Key error, please check. \", item)\n",
    "            continue\n",
    "        \n",
    "        for query, answer in zip(ques, anss):\n",
    "            \n",
    "            docs = knowledge_base.invoke(query)\n",
    "\n",
    "            QA_prompt = QA_user_prompt.format(\"\\n\".join([docs[i].page_content for i in range(len(docs))]), query)\n",
    "            response = gpt_llm(QA_sys_prompt, QA_prompt)\n",
    "            verified_output, verified_bool = response_evaluation(query, answer, response)\n",
    "            \n",
    "            print(\"======================\")\n",
    "            print(f\"Query: {query}\")\n",
    "            print(f\"Ans: {answer}\")\n",
    "            print(f\"Res: {response}\")\n",
    "            print(f\"Correct or not: {verified_bool}\")\n",
    "            correctness.append(verified_bool)\n",
    "            results.append([pdf, query, answer, response, verified_output, verified_bool])\n",
    "            \n",
    "    return correctness, results\n",
    "\n",
    "def response_evaluation(query, answer, response):\n",
    "    verifier_prompt = verifier_user_prompt.format(query, answer, response)\n",
    "    verified_output = gpt_llm(verifier_sys_prompt, verifier_prompt)\n",
    "    print(\"verified_output: \", verified_output)\n",
    "    verified_bool = verified_output.split('\"answer\": ')[-1]\n",
    "    if \"True\" in verified_bool:\n",
    "        verified_bool = 1\n",
    "    else:\n",
    "        verified_bool = 0\n",
    "    return verified_output, verified_bool\n",
    "\n",
    "def post_response_evaluation(df):\n",
    "    \n",
    "    responses = df.loc[:, \"response\"].values\n",
    "    verified_output = df.loc[:, \"verified_output\"].values\n",
    "    \n",
    "    judged_res = []\n",
    "    for res, v_o in zip(responses, verified_output):\n",
    "        verified_bool = v_o.split('\"answer\": ')[-1]\n",
    "        if \"True\" in verified_bool:\n",
    "            verified_bool = 1\n",
    "        else:\n",
    "            verified_bool = 0\n",
    "        if \"Sorry\" in res:\n",
    "            verified_bool = 0\n",
    "        judged_res.append(verified_bool)\n",
    "    return judged_res\n",
    "\n",
    "model_names = [\"BM25\", \"mixedbread-ai/mxbai-embed-large-v1\", \"BAAI/bge-large-en-v1.5\"]\n",
    "chunk_sizes = [500, 1000, 1500]\n",
    "\n",
    "os.makedirs(\"./results\", exist_ok=True)\n",
    "os.makedirs(\"./faiss\", exist_ok=True)\n",
    "os.makedirs(\"./BM25\", exist_ok=True)\n",
    "os.makedirs(\"./chunks\", exist_ok=True)\n",
    "\n",
    "acc_scores = []\n",
    "for chunk_size in chunk_sizes:\n",
    "    \n",
    "    n = \"fused\"\n",
    "    result_csv = f\"./results/qa_full-v2_cs-{chunk_size}_{n}.xlsx\"\n",
    "    if os.path.exists(result_csv):\n",
    "        df = pd.read_excel(result_csv)\n",
    "        correctness = df.loc[:,\"verified_bool\"].values\n",
    "        \n",
    "        judged_res = post_response_evaluation(df)\n",
    "        acc_scores.append(np.mean(judged_res))\n",
    "        continue\n",
    "    \n",
    "    ## Create or load kb\n",
    "    knowledge_base = kb_initialization(model_names, chunk_size)\n",
    "    \n",
    "    ## Get output\n",
    "    # load qa dataset\n",
    "    json_filename = \"QA_dataset_v2.json\"\n",
    "    with open(json_filename, 'r') as file:\n",
    "        data_loaded = json.load(file)\n",
    "\n",
    "    ## Evaluation\n",
    "    correctness, results = get_response_and_evaluation(data_loaded, knowledge_base)\n",
    "        \n",
    "    ## Save results to csv file\n",
    "    df = pd.DataFrame(results, columns = [\"filename\", \"query\", \"answer\", \"response\", \"verified_output\", \"verified_bool\"])\n",
    "    df.to_excel(result_csv, index=False)\n",
    "    \n",
    "    ## Re-calculate the accuracy\n",
    "    judged_res = post_response_evaluation(df)\n",
    "    acc_scores.append(np.mean(judged_res))\n",
    "\n",
    "df_res = pd.DataFrame([acc_scores], index=[\"fused\"], columns=chunk_sizes)\n",
    "print(df_res)     \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "badminton_court_booking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
